@article{Fisher1915,
abstract = {508 Distribution of the Correlation Coeffeients of Samples In the second of these two papers the more difficult problem of the frequency distribution of the correlation coefficient is attempted. For samples of 2 the frequency},
author = {Fisher, R.a. and Fisher, R.a.},
doi = {10.2307/2331838},
file = {:C$\backslash$:/Users/eribu/Downloads/2331838.pdf:pdf},
isbn = {0006-3444},
issn = {00063444},
journal = {Biometrika},
keywords = {article1},
mendeley-tags = {article1},
number = {4},
pages = {507--521},
title = {{Frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population}},
url = {http://biomet.oxfordjournals.org/cgi/reprint/10/4/507.pdf},
volume = {10},
year = {1915}
}
@misc{Fisher1921,
abstract = {This is the second of three papers dealing with the sampling errors of correlation coefficients covering the cases (i) "The frequency distribution of the values of the correlation coeffricient in samples from an indefinitely large population", Biometrika, 1915.},
author = {Fisher, R A},
booktitle = {Metron},
file = {:C$\backslash$:/Users/eribu/Downloads/14.pdf:pdf},
keywords = {article1},
mendeley-tags = {article1},
number = {1-32},
pages = {205--235},
title = {{On the probable error of a coefficient of correlation deduced from a small samlpe}},
volume = {1},
year = {1921}
}
@article{Gayen1951,
author = {Gayen, A. K.},
file = {:C$\backslash$:/Users/eribu/Downloads/2332329.pdf:pdf},
journal = {Biometrika},
keywords = {article1},
mendeley-tags = {article1},
number = {1/2},
pages = {219--247},
title = {{The Frequency Distribution of the Product-Moment Correlation Coefficient in Random Samples of Any Size Drawn from Non-Normal Universes}},
volume = {38},
year = {1951}
}
@article{Hotelling1953,
author = {Hotelling, Harold},
file = {:C$\backslash$:/Users/eribu/Downloads/2983768.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological),},
keywords = {article1},
mendeley-tags = {article1},
number = {2},
pages = {296--193--232},
title = {{New Light on the Correlation Coefficient and its Transforms Author(s): Harold Hotelling}},
volume = {15},
year = {1953}
}
@article{Olkin1958,
abstract = {This paper deals with the unbiased estimation of the correlation of two variates having a bivariate normal distribution (Sec. 2), and of the intraclass correlation, i.e., the common correlation coefficient of a {\$}p{\$}-variate normal distribution with equal variances and equal covariances (Sec. 3). In both cases, the estimator has the following properties. It is a function of a complete sufficient statistic and is therefore the unique (except for sets of probability zero) minimum variance unbiased estimator. Its range is the region of possible values of the estimated quantity. It is a strictly increasing function of the usual estimator differing from it only by terms of order {\$}1/n{\$} and consequently having the same asymptotic distribution. Since the unbiased estimators are cumbersome in form in that they are expressed as series or integrals, tables are included giving the unbiased estimators as functions of the usual estimators. In Sec. 4 we give an unbiased estimator of the squared multiple correlation. It ha...},
author = {Olkin, Ingram and Pratt, J.W.},
doi = {10.2307/2237306},
file = {:C$\backslash$:/Users/eribu/Downloads/2237306.pdf:pdf},
issn = {00034851},
journal = {The annals of mathematical statistics},
keywords = {article1},
mendeley-tags = {article1},
number = {1},
pages = {201--211},
title = {{Unbiased estimation of certain correlation coefficients}},
url = {http://www.jstor.org/pss/2237306$\backslash$nhttp://www.jstor.org/stable/10.2307/2237306},
volume = {29},
year = {1958}
}
@article{Hogben1968,
author = {Hogben, David},
doi = {10.6028/jres.072B.007},
file = {:C$\backslash$:/Users/eribu/Downloads/jresv72Bn1p33{\_}A1b.pdf:pdf},
issn = {0098-8979},
journal = {Journal of Research of the National Bureau of Standards, Section B: Mathematical Sciences},
keywords = {anal ysis of variance,article1,calibrali on,co rrelation coeffi cie,degrees of free dom,di s-,e,fix ed vari abl,nonce ntral beta variabl,noncentrality,nt,q variate,tribution},
mendeley-tags = {article1},
number = {1},
pages = {33},
title = {{The distribution of the sample correlation coefficient with one variable fixed}},
volume = {72B},
year = {1968}
}
@article{Warren1971,
abstract = {A quantitative study is made of the bias in the usual estimate of the linear correlation coefficient and of the relative efficiency of the estimated regression, when a certain type of selective sampling is employed. A good compromise seems difficult to obtain and it is the author's thesis that the simultaneous presentation of regression equations and correlation coefficients is, in a sense, contradictory.},
author = {Warren, W. G.},
doi = {10.2307/2346463},
file = {:C$\backslash$:/Users/eribu/Downloads/2346463.pdf:pdf},
issn = {00359254},
journal = {Applied Statistics},
keywords = {article1},
mendeley-tags = {article1},
number = {2},
pages = {148},
title = {{Correlation or Regression: Bias or Precision}},
url = {http://www.jstor.org/stable/2346463$\backslash$nhttp://www.jstor.org/stable/10.2307/2346463?origin=crossref},
volume = {20},
year = {1971}
}
@article{Kowalski1972,
abstract = {Samples from non-normal bivariate distributions are simulated and the densities of the sample product-moment correlation coefficient, r, estimated and compared with the corresponding normal theory densities. The results are contrasted with the literature on the subject and an attempt is made to reconcile some of the earlier conflicting conclusions regarding the robustness of the distribution of r.},
author = {Kowalski, Charles J.},
doi = {10.2307/2346598},
file = {:C$\backslash$:/Users/eribu/Downloads/2346598.pdf:pdf},
issn = {00359254},
journal = {Journal of the Royal Statistical Society},
keywords = {article1,density estimation,distribution of correlation coefficient,non-normality,robustness,transformations},
mendeley-tags = {article1},
number = {1},
pages = {1--12},
title = {{On the Effects of Non-Normality on the Distribution of the Sample Product-Moment Correlation Coefficient}},
url = {http://www.jstor.org/stable/10.2307/2346598?origin=crossref},
volume = {21},
year = {1972}
}
@article{Konishi1978,
author = {Konishi, Sandnori},
file = {:C$\backslash$:/Users/eribu/Downloads/2335923.pdf:pdf},
journal = {Biometrika},
keywords = {article1},
mendeley-tags = {article1},
number = {3},
pages = {654--656},
title = {{An Approximation to the Distribution of the Sample Correlation Coefficient}},
volume = {65},
year = {1978}
}
@article{Alam1979,
author = {Alam, Kursheed},
file = {:C$\backslash$:/Users/eribu/Downloads/ADA048126.pdf:pdf},
journal = {Naval Research Logistics Quarterl},
keywords = {article1},
mendeley-tags = {article1},
title = {{Distribution of sample correlation coefficients}},
year = {1979}
}
@article{Efron1983,
abstract = {We construct a prediction rule on the basis of some data, and then wish to estimate the error rate of this rule in classifying future observations. Cross-validation provides a nearly unbiased estimate, using only the original data. Cross-validation turns out to be related ... $\backslash$n},
author = {Efron, Bradley},
doi = {10.1080/01621459.1983.10477973},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {anova,article1,bootstrap,decomposition,logistic regression,prediction problem},
mendeley-tags = {article1},
number = {382},
pages = {316},
title = {{Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation}},
url = {http://www.jstor.org/stable/2288636?origin=crossref$\backslash$npapers3://publication/doi/10.2307/2288636},
volume = {78},
year = {1983}
}
@article{Kvalseth1985,
abstract = {Abstract The coefficient of determination (R 2) is perhaps the single most extensively used measure of goodness of fit for regression models. It is also widely misused. The primary source of the problem is that except for linear models with an intercept term, the several alternative R 2 statistics are not generally equivalent. This article discusses various considerations and potential pitfalls in using the R 2's. Specific points are exemplified by means of empirical data. A new resistant statistic is also introduced.},
author = {Kv{\aa}lseth, Tarald O.},
doi = {10.1080/00031305.1985.10479448},
file = {:C$\backslash$:/Users/eribu/Downloads/2683704.pdf:pdf},
isbn = {0003-1305},
issn = {0003-1305},
journal = {American Statistician},
keywords = {article1,coefficient},
mendeley-tags = {article1},
number = {4},
pages = {279--285},
title = {{Cautionary Note about R2}},
url = {http://dx.doi.org/10.1080/00031305.1985.10479448},
volume = {39},
year = {1985}
}
@article{Hawkins1989,
abstract = {Hogg, RV, and Craig, AT (1978), Introduction to Mathematical Sta- tistics (4th ed.), New York: Macmillan. Pitman, EG (1939), "A Note on Normal CorTelation," Biomnetrika, 31, 9-12. Snedecor, GW, and Cochran, WG (1980), Statistical Methods (7th ed.), Ames: Iowa State},
author = {Hawkins, D},
doi = {10.2307/2685369},
file = {:C$\backslash$:/Users/eribu/Downloads/2685369.pdf:pdf},
issn = {00031305},
journal = {American Statistician},
keywords = {article1},
mendeley-tags = {article1},
number = {4},
pages = {235--237},
title = {{Using U statistics to derive the asymptotic distribution of Fisher's Z statistic}},
url = {http://www.jstor.org/stable/2685369$\backslash$npapers3://publication/uuid/25473EA2-360A-4031-870D-0A90BCE030FF},
volume = {43},
year = {1989}
}
@article{Nagelkerke1991,
abstract = {A generalization of the coefficient of determination R2 to general regression models is discussed. A modification of an earlier definition to allow for discrete models is proposed.},
author = {Nagelkerke, N. J D},
doi = {10.1093/biomet/78.3.691},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagelkerke - 1991 - A note on a general definition of the coefficient of determination.pdf:pdf},
isbn = {0006-3444},
issn = {00063444},
journal = {Biometrika},
keywords = {Discrete probability,Log likelihood,Multiple correlation coefficient,Regression model,Residual variation,article1},
mendeley-tags = {article1},
number = {3},
pages = {691--692},
pmid = {339},
title = {{A note on a general definition of the coefficient of determination}},
volume = {78},
year = {1991}
}
@article{Efron1997,
abstract = {A study investigates the error rate of a rule for predicting future responses constructed from a training set of data. Results are nonparametric and apply to any possible prediction rule.},
annote = {Handlar om Bootstrap .632+ (vilket kanske {\"{a}}nnu inte {\"{a}}r helt relevant f{\"{o}}r oss).

Utv{\"{a}}rderar p{\aa} 24 olika modeller presenteade i tabell. Varierar sample size och dimensionality som vi. Dock stor skillnad att det baseras p{\aa} classification och inte regression.

Anv{\"{a}}nder mindre samlpe sizes {\"{a}}n vi och fler variabler (st{\"{o}}rre p).

Har inte l{\"{a}}st s{\aa} noggrant d{\aa} den inte k{\"{a}}nns helt relevant och {\"{a}}r ganska teoretisk med mkt notation etc.},
author = {Efron, B. and Tibshirani, R.},
doi = {10.1080/01621459.1997.10474007},
file = {:C$\backslash$:/Users/eribu/Downloads/01621459{\%}2E1997{\%}2E10474007.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {article1,classification,cross-validation bootstrap,prediction rule},
mendeley-tags = {article1},
number = {438},
pages = {548},
pmid = {370},
title = {{Improvements on cross-validation: The .632 plus bootstrap method}},
volume = {92},
year = {1997}
}
@article{ShigekazuNakagawa1997,
author = {{Shigekazu Nakagawa}, Naoto Niki},
file = {:C$\backslash$:/Users/eribu/Downloads/110001235586.pdf:pdf},
keywords = {article1,mixed conductors,p-t-x,perovskites,thermodynamic stability},
mendeley-tags = {article1},
number = {97},
title = {{Distribution of the sample correlation coefficient for nonnormal populations}},
volume = {2738},
year = {1997}
}
@article{Jr1998,
author = {Jr, Frank E Harrell},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jr - 1998 - Comparison of Strategies for Validating Binary Logistic Regression Models.pdf:pdf},
journal = {Statistics},
keywords = {article1},
mendeley-tags = {article1},
title = {{Comparison of Strategies for Validating Binary Logistic Regression Models}},
year = {1998}
}
@article{Breiman2001,
abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commit-ment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current prob-lems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
annote = {Denna artikel {\"{a}}r kritisk mot modellering och f{\"{o}}redrar i likhet med Kuhn att utg{\aa} fr{\aa}n accuracy meassure, inte model validering, d{\aa} detta ofta inte l{\aa}ter sig g{\"{o}}ras s{\aa} l{\"{a}}tt.

Inneh{\aa}ller ocks{\aa} kommentarer fr{\aa}n ett flertal auktoriteter p{\aa} omr{\aa}det samt d{\"{a}}refter ett avslutande svar fr{\aa}n f{\"{o}}rfattaren.},
author = {Breiman, Leo},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf:pdf},
journal = {Statistical Science},
keywords = {article1,statistics},
mendeley-tags = {article1,statistics},
number = {3},
pages = {199--231},
title = {{Statistical Modeling: The Two Cultures}},
volume = {16},
year = {2001}
}
@article{Steyerberg2001,
abstract = {The performance of a predictive model is overestimated when simply determined on the sample of subjects that was used to construct the model. Several internal validation methods are available that aim to provide a more accurate estimate of model performance in new subjects. We evaluated several variants of split-sample, cross-validation and bootstrapping methods with a logistic regression model that included eight predictors for 30-day mortality after an acute myocardial infarction. Random samples with a size between n = 572 and n = 9165 were drawn from a large data set (GUSTO-I; n = 40,830; 2851 deaths) to reflect modeling in data sets with between 5 and 80 events per variable. Independent performance was determined on the remaining subjects. Performance measures included discriminative ability, calibration and overall accuracy. We found that split-sample analyses gave overly pessimistic estimates of performance, with large variability. Cross-validation on 10{\%} of the sample had low bias and low variability, but was not suitable for all performance measures. Internal validity could best be estimated with bootstrapping, which provided stable estimates with low bias. We conclude that split-sample validation is inefficient, and recommend bootstrapping for estimation of internal validity of a predictive logistic regression model.},
annote = {Tipsad av Szilard.

fokuserad p{\aa} logistisk regression.

Utg{\aa}r fr{\aa}n praktiskt exempel med hj{\"{a}}rtattack.
V{\aa}r ansats {\"{a}}r mer teoretisk/generell.
J{\"{a}}mf{\"{o}}r med olika EPV-v{\"{a}}rden och stratifiering.
Resamplar 500 ggr.
stepwise model selection med 8 predektorer.

Utv{\"{a}}rderar flera olika m{\"{a}}tv{\"{a}}rden, inkl R2 (dock Negelkerke).

Olika metoder men inte jack-knife.

F{\aa}r delvis samma slutsats som vi ang {\"{o}}verskattning med cv.},
author = {Steyerberg, Ewout W and Harrell, Frank E and Borsboom, Gerard J.J.M and Eijkemans, M.J.C and Vergouwe, Yvonne and Habbema, J.Dik F},
doi = {10.1016/S0895-4356(01)00341-9},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Steyerberg et al. - 2001 - Internal validation of predictive models.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Bootstrapping,Internal validation,Logistic regression analysis,Predictive models,article1,harrell,statistics},
mendeley-tags = {article1,harrell,statistics},
month = {aug},
number = {8},
pages = {774--781},
title = {{Internal validation of predictive models}},
url = {http://www.sciencedirect.com/science/article/pii/S0895435601003419},
volume = {54},
year = {2001}
}
@article{Fu2005,
abstract = {MOTIVATION: Estimation of misclassification error has received increasing attention in clinical diagnosis and bioinformatics studies, especially in small sample studies with microarray data. Current error estimation methods are not satisfactory because they either have large variability (such as leave-one-out cross-validation) or large bias (such as resubstitution and leave-one-out bootstrap). While small sample size remains one of the key features of costly clinical investigations or of microarray studies that have limited resources in funding, time and tissue materials, accurate and easy-to-implement error estimation methods for small samples are desirable and will be beneficial.$\backslash$n$\backslash$nRESULTS: A bootstrap cross-validation method is studied. It achieves accurate error estimation through a simple procedure with bootstrap resampling and only costs computer CPU time. Simulation studies and applications to microarray data demonstrate that it performs consistently better than its competitors. This method possesses several attractive properties: (1) it is implemented through a simple procedure; (2) it performs well for small samples with sample size, as small as 16; (3) it is not restricted to any particular classification rules and thus applies to many parametric or non-parametric methods.},
author = {Fu, Wenjiang J. and Carroll, Raymond J. and Wang, Suojin},
doi = {10.1093/bioinformatics/bti294},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu, Carroll, Wang - 2005 - Estimating misclassification error with small samples via bootstrap cross-validation.pdf:pdf},
isbn = {1367-4803 (Print)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
keywords = {article1},
mendeley-tags = {article1},
pmid = {15691862},
title = {{Estimating misclassification error with small samples via bootstrap cross-validation}},
year = {2005}
}
@article{Bioinformatics2006,
abstract = {Background: Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data. Results: We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" and "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error. The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30{\%} on 18.5{\%} of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30{\%} on 38{\%} of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance. The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions. Conclusion: We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.},
author = {Bioinformatics, Bmc and Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bioinformatics, Varma, Simon - 2006 - Bias in error estimation when using cross-validation for model selection.pdf:pdf},
journal = {BMC Bioinformatics},
keywords = {article1},
mendeley-tags = {article1},
number = {7},
title = {{Bias in error estimation when using cross-validation for model selection}},
url = {http://www.biomedcentral.com/1471-2105/7/91},
volume = {7},
year = {2006}
}
@article{Isaksson2008,
abstract = {The interest in statistical classification for critical applications such as diagnoses of patient samples based on supervised learning is rapidly growing. To gain acceptance in applications where the subsequent decisions have serious consequences, e.g. choice of cancer therapy, any such decision support system must come with a reliable performance estimate. Tailored for small sample problems, cross-validation (CV) and bootstrapping (BTS) have been the most commonly used methods to determine such estimates in virtually all branches of science for the last 20 years. Here, we address the often overlooked fact that the uncertainty in a point estimate obtained with CV and BTS is unknown and quite large for small sample classification problems encountered in biomedical applications and elsewhere. To avoid this fundamental problem of employing CV and BTS, until improved alternatives have been established, we suggest that the final classification performance always should be reported in the form of a Bayesian confidence interval obtained from a simple holdout test or using some other method that yields conservative measures of the uncertainty. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Isaksson, A. and Wallman, M. and G??ransson, H. and Gustafsson, M. G.},
doi = {10.1016/j.patrec.2008.06.018},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Isaksson et al. - 2008 - Cross-validation and bootstrapping are unreliable in small sample classification.pdf:pdf},
isbn = {01678655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Confidence interval,Performance estimation,Supervised classification,article1},
mendeley-tags = {article1},
title = {{Cross-validation and bootstrapping are unreliable in small sample classification}},
year = {2008}
}
@article{Kim2009,
abstract = {We consider the accuracy estimation of a classifier constructed on a given training sample. The naive resubstitution estimate is known to have a downward bias problem. The traditional approach to tackling this bias problem is cross-validation. The bootstrap is another way to bring down the high variability of cross-validation. But a direct comparison of the two estimators, cross-validation and bootstrap, is not fair because the latter estimator requires much heavier computation. We performed an empirical study to compare the??.632+??bootstrap estimator with the repeated 10-fold cross-validation and the repeated one-third holdout estimator. All the estimators were set to require about the same amount of computation. In the simulation study, the repeated 10-fold cross-validation estimator was found to have better performance than the??.632+??bootstrap estimator when the classifier is highly adaptive to the training sample. We have also found that the??.632+??bootstrap estimator suffers from a bias problem for large samples as well as for small samples. ?? 2009 Elsevier B.V. All rights reserved.},
annote = {behandlar classification, inte regression.},
author = {Kim, Ji Hyun},
doi = {10.1016/j.csda.2009.04.009},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim - 2009 - Estimating classification error rate Repeated cross-validation, repeated hold-out and bootstrap.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {article1},
mendeley-tags = {article1},
number = {11},
pages = {3735--3745},
title = {{Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap}},
volume = {53},
year = {2009}
}
@article{Gorsuch2010,
abstract = {Non-zero correlation coefficients have non-normal distributions, affecting both means and standard deviations. Previous research suggests that z transformation may effectively correct mean bias for N's less than 30. In this study, simulations with small (20 and 30) and large (50 and 100) N's found that mean bias adjustments for larger N's are seldom needed. However, z transformations improved confidence intervals even for N = 100. The improvement was not in the estimated standard errors so much as in the asymmetrical CI's estimates based upon the z transformation. The resulting observed probabilities were generally accurate to within 1 point in the first non-zero digit. These issues are an order of magnitude less important for accuracy than design issues influencing the accuracy of the results, such as reliability, restriction of range, and N. Keywords: Confidence intervals; Correlation coefficient; Fisherâ€™s z transformation; Monte Carlo study; Mean bias in correlation coefficients},
author = {Gorsuch, Rl and Lehmann, Cs},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gorsuch, Lehmann - 2010 - Correlation Coefficients Mean Bias and Confidence Interval Distortions.pdf:pdf},
issn = {2159-7855},
journal = {Journal of Methods and Measurement in the Social Sciences},
keywords = {article1,because the distribution of,coefficients,confidence intervals,correlation coefficient,estimate the population correlation,fisher,is known to slightly,mean bias in correlation,monte carlo,r,r is,s z transformation,study,the observed correlation coefficient,under,$\rho$},
mendeley-tags = {article1},
number = {2},
pages = {52--65},
title = {{Correlation Coefficients: Mean Bias and Confidence Interval Distortions}},
url = {https://journals.uair.arizona.edu/index.php/jmmss/article/download/114/118},
volume = {1},
year = {2010}
}
@article{Ruben2010,
author = {Ruben, Harold and Journal, Source and Statistical, Royal and Series, Society},
file = {:C$\backslash$:/Users/eribu/Downloads/2984445.pdf:pdf},
keywords = {article1},
mendeley-tags = {article1},
number = {3},
pages = {513--525},
title = {{Some New Results on the Coefficient of the Sample Correlation Distribution}},
volume = {28},
year = {2010}
}
@article{Society2013,
author = {Society, The Econometric},
file = {:C$\backslash$:/Users/eribu/Downloads/1909612.pdf:pdf},
keywords = {article1},
mendeley-tags = {article1},
number = {1},
pages = {187--189},
title = {{The Distribution of the Sample Correlation Coefficient Under the Null Hypothesis Author ( s ): Kern O . Kymn Published by : The Econometric Society Stable URL : http://www.jstor.org/stable/1909612 .}},
volume = {36},
year = {2013}
}
@article{Hankin,
abstract = {This paper introduces the hypergeo package of R routines for numerical calculation of hypergeometric functions. The package is focussed on efficient and accurate evaluation of the Gauss hypergeometric function over the whole of the complex plane within the constraints of fixed-precision arithmetic. The hypergeometric series is convergent only within the unit circle, so analytic continuation must be used to define the function outside the unit circle. This short document outlines the numerical and conceptual methods used in the package; and justifies the package philosophy, which is to maintain transparent and verifiable links between the software and Abramowitz and Stegun (1965). Most of the package functionality is accessed via the single function hypergeo(), which dispatches to one of several methods depending on the value of its arguments. The package is demonstrated in the context of game theory.},
author = {Hankin, Robin K S},
file = {:C$\backslash$:/Users/eribu/Downloads/hypergeometric.pdf:pdf},
issn = {20734859},
keywords = {article1,complex plane,hypergeometric functions,numerical evaluation,r,residue theo-},
mendeley-tags = {article1},
number = {1},
pages = {1--7},
title = {{Numerical evaluation of the Gauss hypergeometric function with the hypergeo package}}
}
